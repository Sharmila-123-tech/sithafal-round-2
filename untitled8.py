# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PXugZnNW96ARjhThA8ZcKVA1Ol5CS179
"""

!pip install PyPDF2 sentence-transformers faiss-cpu langchain openai

!pip install langchain_community

!mkdir pdf_files

pdf_folder = "/content/pdf_files/pdf.files.pdf"
pdf_files = "pdf.files.pdf"

def extract_text_from_pdf(pdf_path):
    """
    Extracts text content from a PDF file.
    """
    text = ""
    with open(pdf_path, "rb") as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

def chunk_text(text, chunk_size=500):
    """
    Splits text into chunks of a specified size.
    """
    words = text.split()
    return [" ".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]

def process_pdf_files(pdf_files):
    """
    Processes multiple PDF files, extracting and chunking text.
    """
    all_chunks = []
    for file_path in pdf_files:
        print(f"Processing file: {file_path}")
        text = extract_text_from_pdf(file_path)
        chunks = chunk_text(text)
        all_chunks.extend(chunks)
    return all_chunks

def create_faiss_index(chunks, embedding_model_name="all-MiniLM-L6-v2"):
    """
    Embeds text chunks and stores them in a FAISS vector database.
    """
    # Load embedding model
    embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)

    # Check if chunks is empty
    if not chunks:
        print("No text chunks found. Please make sure there are PDF files and they contain text.")
        return None  # Or handle the empty case appropriately

    # Convert chunks into embeddings and store in FAISS
    vector_store = FAISS.from_texts(chunks, embeddings)
    return vector_store

def compare_query(vector_store, query_terms,llm_api_key):
    """
    Handles comparison queries by retrieving relevant data and formatting a structured response.
    """
    responses = []
    for term in query_terms:
        response = answer_query(vector_store, term, llm_api_key)
        responses.append(f"{term}: {response}")

    # Aggregate results for comparison
    comparison_result = "\n".join(responses)
    return comparison_result

if __name__ == "__main__":

    # Step 1: Process PDF files
    pdf_files = [pdf_folder]
    chunks = process_pdf_files(pdf_files)

    # Step 2: Create FAISS vector store
    print("Creating FAISS index...")
    vector_store = create_faiss_index(chunks)

    # Step 3: Handle user queries
    print("\nInteractive Chat with PDFs:")
    while True:
        print("\nType 'exit' to stop.")
        query = input("Enter your query: ")
        if query.lower() == "exit":
            break

        # Check if query indicates a comparison
        if "compare" in query.lower():
            terms = query.replace("compare", "").split("and")
            terms = [t.strip() for t in terms]
            response = compare_query(vector_store, terms, llm_api_key)
        else:
            response = answer_query(vector_store, query, llm_api_key)

        print(f"\nResponse:\n{response}")